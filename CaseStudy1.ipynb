{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1 : Collecting Data from Twitter\n",
    "\n",
    "Due Date: September 22, **before the beginning of class at 6:00pm**\n",
    "\n",
    "* ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/9/9f/Twitter_bird_logo_2012.svg/220px-Twitter_bird_logo_2012.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
    "\n",
    "    member 1\n",
    "    \n",
    "    member 2\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* Chapter 1 and Chapter 9 of the book [Mining the Social Web](http://www.learndatasci.com/wp-content/uploads/2015/08/Mining-the-Social-Web-2nd-Edition.pdf) \n",
    "* The codes for [Chapter 1](http://bit.ly/1qCtMrr) and [Chapter 9](http://bit.ly/1u7eP33)\n",
    "\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Sampling Twitter Data with Streaming API about a certain topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select a topic that you are interested in, for example, \"WPI\" or \"Lady Gaga\"\n",
    "* Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million.\n",
    "* Store the tweets you downloaded into a local file (txt file or json file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import time\n",
    "import json\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API\n",
    "consumer_key = \"1edf1EkmplIy7bSUR5j6ODzGu\"\n",
    "consumer_secret = \"QDGNRSHkzES2Qrexpln5Uhj4viJ3UyJw9T0lmoC8UgG2zJaDJY\"\n",
    "access_token = \"3856128089-RgsqT2MPPeCtSaPKXMzJMX1Y603FpDipGSZhOGf\"\n",
    "access_token_secret = \"fdFDkN6fn68a7DUFYGB3auP8r3PVQPHAYrZiWdVYOmdwK\"\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            all_data = json.loads(data)\n",
    "            tweet = all_data[\"text\"]\n",
    "            print(tweet)\n",
    "            print(data)\n",
    "            saveFile = open('tweets.json','a')\n",
    "            saveFile.write(data)\n",
    "            #saveFile.write('\\n')\n",
    "            saveFile.close()\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print(('failed ondata',str(e)))\n",
    "            time.sleep(5)\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'Trump2016', 'Hillary2016'\n",
    "    stream.filter(track=['Trump2016', 'Hillary2016','#imwithher','#makeamericagreatagain','election2016','Donald Trump','Hillary Clinton','presidentialelections2016','donald trump','hillary clinton'])#imwithher ##makeamericagreatagain\n",
    "    # g = st[\"text\"]\n",
    "    # print(g)\n",
    "\n",
    "# Run this python file on command line to store data in .txt or .json file: ' python twitter_streaming.py > twitter_data.json '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report some statistics about the tweets you collected \n",
    "\n",
    "* The topic of interest: 2016 Presidential Elections\n",
    "\n",
    "\n",
    "* The total number of tweets collected:  127,367"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Analyzing Tweets and Tweet Entities with Frequency Analysis\n",
    "\n",
    "**1. Word Count:** \n",
    "* Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets. \n",
    "* Plot a table of the top 30 words with their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each blocimport json\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from prettytable import PrettyTable\n",
    "import nltk\n",
    "\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "wordss = []\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "fname = 'tweets.json'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet1 = json.loads(line)     \n",
    "        terms_all = [term for term in preprocess(tweet1['text'])]\n",
    "        for w in terms_all:\n",
    "            w = nltk.word_tokenize(w)\n",
    "            w = nltk.pos_tag(w)\n",
    "            print(w)\n",
    "            if w[0][1] in ['JJ'] and w[0][0] not in stopwords.words(\"english\") and w[0][0] not in['DON','@','Hillary','RT','http','https','Obama','Bill','It''.',',','--','\\'s','?',')','(',':','\\'','\\'re','\"','-','}','{',\n",
    "u'‚Äî','','RT','‚Ä¶','!','?','‚Äô','I','https','Make','It','says',\"Let's\",'told','Happen',';','The','/','&',':/','amp','via'\n",
    ",'#',':','\"','‚Äú','‚Äù','|','??','$','hillary','Trump','Clinton','Obama','Is','üí•','it','It','%','htt','-','‚Äò','üá∫','üá∏','üòÇ']:wordss.append(w[0][0]) \n",
    "\n",
    "\n",
    "for item in wordss:\n",
    "    c = Counter(item)\n",
    "\n",
    "print(Counter(wordss).most_common(30))\n",
    "\n",
    "\n",
    "for data in (wordss):\n",
    "    pt = PrettyTable(field_names=['WORD', 'Count']) \n",
    "    c = Counter(wordss)\n",
    "    [ pt.add_row(kv) for kv in c.most_common()[:30] ]\n",
    "    pt.align['wordss'], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "print (pt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the most popular tweets in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "popular_tweets= PrettyTable([\"Count\",\"Popular Tweets\"])\n",
    "with open('tweets.json','r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        #print(tweet)\n",
    "        tweet_text = [tweet['text']]\n",
    "        #print(terms_all)\n",
    "        count_all.update(tweet_text)\n",
    "    #print(count_all.most_common(10))\n",
    "    x=count_all.most_common(10)\n",
    "    for item in x:\n",
    "        popular_tweets.add_row(item)\n",
    "    popular_tweets.align[\"Popular Tweets\"] ='l'\n",
    "    popular_tweets.align[\"Count\"]='l'\n",
    "    popular_tweets._max_width[\"Popular Tweets\"]=50\n",
    "    print(popular_tweets)\n",
    "\n",
    "    with open('popularTweets.txt','w',encoding='utf-8')as w:\n",
    "        w.write(str(popular_tweets))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Find the most popular Tweet Entities in your collection of tweets**\n",
    "\n",
    "Please plot a table of the top 10 hashtags, top 10 user mentions that are the most popular in your collection of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "# Done Using MONGODB and ELASTIC SEARCH, KIBANA (Problem 4)\n",
    "\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from elasticsearch import Elasticsearch\n",
    "# create instance of elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "count = 0\n",
    "with open('outfile.json','r') as f:\n",
    "    #count_all = Counter()\n",
    "    for line in f:\n",
    "        dict_data = json.loads(line)\n",
    "        #x = type(dict_data)\n",
    "\n",
    "        #pass tweet into TextBlob\n",
    "        tweet = TextBlob(dict_data[\"text\"])\n",
    "        # print(tweet)\n",
    "\n",
    "        # output sentiment polarity\n",
    "        #print (tweet.sentiment.polarity)\n",
    "\n",
    "        #determine if sentiment is positive, negative, or neutral\n",
    "        if tweet.sentiment.polarity < 0:\n",
    "            sentiment = 'negative'\n",
    "        elif tweet.sentiment.polarity == 0:\n",
    "            sentiment = \"neutral\"\n",
    "        else:\n",
    "            sentiment = \"positive\"\n",
    "\n",
    "        # output sentiment\n",
    "        #print (sentiment)\n",
    "        #es.put_script\n",
    "        #add text and sentiment info to elasticsearch\n",
    "        es.index(index=\"twitter\",\n",
    "                 doc_type=\"tweets\",#\n",
    "                 body={\"author\": dict_data[\"user\"][\"screen_name\"],\n",
    "                       \"date\": dict_data[\"created_at\"],\n",
    "                       \"message\": dict_data[\"text\"],\n",
    "                       #\"geo\":dict_data[\"geo\"],\n",
    "                       #\"coordinates\":dict_data[\"coordinates\"],\n",
    "                       \"place\":dict_data[\"place\"],\n",
    "                       \"retweet count\":dict_data[\"retweet_count\"],\n",
    "                       \"favorite count\":dict_data[\"favorite_count\"],\n",
    "                       #\"timestamp\":dict_data[\"timestamp_ms\"],\n",
    "                       \"polarity\": tweet.sentiment.polarity,\n",
    "                       \"subjectivity\": tweet.sentiment.subjectivity,\n",
    "                       \"sentiment\": sentiment,\n",
    "                       \"User Name\":dict_data[\"user\"][\"name\"],\n",
    "                       \"User Location\":dict_data[\"user\"][\"location\"],\n",
    "                       \"User Follower Count\":dict_data[\"user\"][\"followers_count\"],\n",
    "                       \"User Friend Count\":dict_data[\"user\"][\"friends_count\"],\n",
    "                       \"User Created Date\":dict_data[\"user\"][\"created_at\"],\n",
    "                       \"hashtags\":dict_data[\"entities\"][\"hashtags\"],\n",
    "                       \"user_mentions\":dict_data[\"entities\"][\"user_mentions\"],\n",
    "                       \"coordinates\":dict_data[\"coordinates\"]\n",
    "                         }\n",
    "\n",
    "                 )\n",
    "        print(\"done indexing tweet\")\n",
    "print(\"COMPLETE\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #print(dict_data)\n",
    "        #line = json.loads(line)\n",
    "        #terms_all = [term for term in tweet['text']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ------------------------\n",
    "\n",
    "# Problem 3: Getting \"All\" friends and \"All\" followers of a popular user in twitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "* Get the list of all friends and all followers of the twitter user.\n",
    "* Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "* Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from prettytable import PrettyTable\n",
    "import time\n",
    "\n",
    "consumer_key = \"1edf1EkmplIy7bSUR5j6ODzGu\"\n",
    "consumer_secret = \"QDGNRSHkzES2Qrexpln5Uhj4viJ3UyJw9T0lmoC8UgG2zJaDJY\"\n",
    "access_token = \"3856128089-RgsqT2MPPeCtSaPKXMzJMX1Y603FpDipGSZhOGf\"\n",
    "access_token_secret = \"fdFDkN6fn68a7DUFYGB3auP8r3PVQPHAYrZiWdVYOmdwK\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "    screen_name ='USAforTrump2016'  #'USAforTrump2016'  #\"MakeAmericaGre4\", \"TrumpVsHilary\"\n",
    "\n",
    "    user = api.get_user(screen_name)\n",
    "    print('Screen Name -',user.screen_name)\n",
    "    print('Description -',user.description)\n",
    "    print('Followers -',user.followers_count)\n",
    "    print('Status Count -',user.statuses_count)\n",
    "    print('User URL -',user.url)\n",
    "\n",
    "    # Extracting list of all friends and followers\n",
    "    friends = api.friends_ids(screen_name)\n",
    "    #print('Friends',friends)\n",
    "    #print(len(friends))\n",
    "\n",
    "    followers = []\n",
    "    for page in tweepy.Cursor(api.followers_ids, screen_name='USAforTrump2016').pages():\n",
    "        followers.extend(page)\n",
    "        time.sleep(70)\n",
    "    #print(len(followers),'- followers')\n",
    "\n",
    "\n",
    "    friend_id = friends[1:21]\n",
    "    #print(friend_id,'friend_id')\n",
    "    friend_name = map(lambda x: api.get_user(x).screen_name,friend_id)\n",
    "    #print(friend_name,'friend_name')\n",
    "\n",
    "    follower_id = followers[1:21]\n",
    "    #print(follower_id,'follower_id')\n",
    "    follower_name = map(lambda x:api.get_user(x).screen_name,follower_id)\n",
    "    #print(follower_name,'follower_name')\n",
    "\n",
    "    # Creating a Pretty Table for top 20 Friends and Followers\n",
    "    friend_pretty = PrettyTable([\"id\",\"name\"])\n",
    "    for item in (friend_id)[1:21]:\n",
    "        friend_pretty.add_row([item,api.get_user(item).screen_name])\n",
    "    print(friend_pretty)\n",
    "\n",
    "    follower_pretty = PrettyTable([\"id\",\"name\"])\n",
    "    for item in (follower_id)[1:21]:\n",
    "        follower_pretty.add_row([item,api.get_user(item).screen_name])\n",
    "    print(follower_pretty)\n",
    "\n",
    "    # For finding Mutual Friends\n",
    "    mutual_id = set(friends).intersection(set(followers))\n",
    "    #print(len(mutual_id))\n",
    "    mutualFriends_pretty = PrettyTable([\"id\",\"name\"])\n",
    "    for item in list(mutual_id)[1:40]:\n",
    "        mutualFriends_pretty.add_row([item,api.get_user(item).screen_name])\n",
    "    print(mutualFriends_pretty,'mutual friends')\n",
    "\n",
    "    # Write the Tables to a file\n",
    "    #time.sleep(70)\n",
    "    with open('friend.txt','w')as w:\n",
    "        w.write(str(friend_pretty))\n",
    "\n",
    "    with open('follower.txt','w')as w:\n",
    "        w.write(str(follower_pretty))\n",
    "\n",
    "    with open('mutualFriends.txt','w')as w:\n",
    "        w.write(str(mutualFriends_pretty))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "\n",
    "# CODE in [2] above\n",
    "\n",
    " mutual_id = set(friends).intersection(set(followers))\n",
    "    #print(len(mutual_id))\n",
    "    mutualFriends_pretty = PrettyTable([\"id\",\"name\"])\n",
    "    for item in list(mutual_id)[1:40]:\n",
    "        mutualFriends_pretty.add_row([item,api.get_user(item).screen_name])\n",
    "    print(mutualFriends_pretty,'mutual friends')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*------------------------\n",
    "\n",
    "# Problem 4: Business question \n",
    "\n",
    "Run some additional experiments with your data to gain familiarity with the twitter data and twitter API.\n",
    "\n",
    "* Come up with a business question that Twitter data could help answer.\n",
    "* Decribe the business case.\n",
    "* How could Twitter data help a company decide how to spend its resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from elasticsearch import Elasticsearch\n",
    "# create instance of elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "count = 0\n",
    "with open('outfile.json','r') as f:\n",
    "    #count_all = Counter()\n",
    "    for line in f:\n",
    "        dict_data = json.loads(line)\n",
    "        #x = type(dict_data)\n",
    "\n",
    "        #pass tweet into TextBlob\n",
    "        tweet = TextBlob(dict_data[\"text\"])\n",
    "        # print(tweet)\n",
    "\n",
    "        # output sentiment polarity\n",
    "        #print (tweet.sentiment.polarity)\n",
    "\n",
    "        #determine if sentiment is positive, negative, or neutral\n",
    "        if tweet.sentiment.polarity < 0:\n",
    "            sentiment = 'negative'\n",
    "        elif tweet.sentiment.polarity == 0:\n",
    "            sentiment = \"neutral\"\n",
    "        else:\n",
    "            sentiment = \"positive\"\n",
    "\n",
    "        # output sentiment\n",
    "        #print (sentiment)\n",
    "        #es.put_script\n",
    "        #add text and sentiment info to Elasticsearch and Kibana\n",
    "        es.index(index=\"twitter\",\n",
    "                 doc_type=\"tweets\",#\n",
    "                 body={\"author\": dict_data[\"user\"][\"screen_name\"],\n",
    "                       \"date\": dict_data[\"created_at\"],\n",
    "                       \"message\": dict_data[\"text\"],\n",
    "                       #\"geo\":dict_data[\"geo\"],\n",
    "                       #\"coordinates\":dict_data[\"coordinates\"],\n",
    "                       \"place\":dict_data[\"place\"],\n",
    "                       \"retweet count\":dict_data[\"retweet_count\"],\n",
    "                       \"favorite count\":dict_data[\"favorite_count\"],\n",
    "                       #\"timestamp\":dict_data[\"timestamp_ms\"],\n",
    "                       \"polarity\": tweet.sentiment.polarity,\n",
    "                       \"subjectivity\": tweet.sentiment.subjectivity,\n",
    "                       \"sentiment\": sentiment,\n",
    "                       \"User Name\":dict_data[\"user\"][\"name\"],\n",
    "                       \"User Location\":dict_data[\"user\"][\"location\"],\n",
    "                       \"User Follower Count\":dict_data[\"user\"][\"followers_count\"],\n",
    "                       \"User Friend Count\":dict_data[\"user\"][\"friends_count\"],\n",
    "                       \"User Created Date\":dict_data[\"user\"][\"created_at\"],\n",
    "                       \"hashtags\":dict_data[\"entities\"][\"hashtags\"],\n",
    "                       \"user_mentions\":dict_data[\"entities\"][\"user_mentions\"],\n",
    "                       \"coordinates\":dict_data[\"coordinates\"]\n",
    "                         }\n",
    "\n",
    "                 )\n",
    "        print(\"done indexing tweet\")\n",
    "print(\"COMPLETE\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #print(dict_data)\n",
    "        #line = json.loads(line)\n",
    "        #terms_all = [term for term in tweet['text']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
    "\n",
    "* ** Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
    "    * What data you collected? \n",
    "    * Why this topic is interesting or important to you? (Motivations)\n",
    "    * How did you analyse the data?\n",
    "    * What did you find in the data? \n",
    " \n",
    "     (please include figures or tables in the report, but no source code)\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through email to Prof. Paffenroth (rcpaffenroth@wpi.edu) *and* the TA Wen Liu (wliu3@wpi.edu).\n",
    "        \n",
    "** Note: Each team just needs to submits one submission **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading Criteria:\n",
    "\n",
    "** Totoal Points: 120 **\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Notebook:  **\n",
    "    Points: 80\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Qestion 1:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) Select a topic that you are interested in.\n",
    "    Points: 6 \n",
    "    \n",
    "    (2) Use Twitter Streaming API to sample a collection of tweets about this topic in real time. (It would be recommended that the number of tweets should be larger than 200, but smaller than 1 million. Please check whether the total number of tweets collected is larger than 200?\n",
    "    Points: 10 \n",
    "    \n",
    "    \n",
    "    (3) Store the tweets you downloaded into a local file (txt file or json file)\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 2:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    1. Word Count\n",
    "\n",
    "    (1) Use the tweets you collected in Problem 1, and compute the frequencies of the words being used in these tweets.\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Plot a table of the top 30 words with their counts \n",
    "    Points: 4 \n",
    "    \n",
    "    2. Find the most popular tweets in your collection of tweets\n",
    "    plot a table of the top 10 tweets that are the most popular among your collection, i.e., the tweets with the largest number of retweet counts.\n",
    "    Points: 4 \n",
    "    \n",
    "    3. Find the most popular Tweet Entities in your collection of tweets\n",
    "\n",
    "    (1) plot a table of the top 10 hashtags, \n",
    "    Points: 4 \n",
    "\n",
    "    (2) top 10 user mentions that are the most popular in your collection of tweets.\n",
    "    Points: 4 \n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    Qestion 3:\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "    \n",
    "    (1) choose a popular twitter user who has many followers, such as \"ladygaga\".\n",
    "    Points: 4 \n",
    "\n",
    "    (2) Get the list of all friends and all followers of the twitter user.\n",
    "    Points: 4 \n",
    "\n",
    "    (3) Plot 20 out of the followers, plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "\n",
    "    (4) Plot 20 out of the friends (if the user has more than 20 friends), plot their ID numbers and screen names in a table.\n",
    "    Points: 4 \n",
    "    \n",
    "    (5) Compute the mutual friends within the two groups, i.e., the users who are in both friend list and follower list, plot their ID numbers and screen names in a table\n",
    "    Points: 4 \n",
    "  \n",
    "    -----------------------------------\n",
    "    Qestion 4:  Business question\n",
    "    Points: 20\n",
    "    -----------------------------------\n",
    "        Novelty: 10\n",
    "        Interestingness: 10\n",
    "    -----------------------------------\n",
    "    Run some additional experiments with your data to gain familiarity with the twitter data ant twitter API.  Come up with a business question and describe how Twitter data can help you answer that question.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Report: communicate the results**\n",
    "    Points: 20\n",
    "\n",
    "(1) What data you collected?\n",
    "    Points: 5 \n",
    "\n",
    "(2) Why this topic is interesting or important to you? (Motivations)\n",
    "    Points: 5 \n",
    "\n",
    "(3) How did you analyse the data?\n",
    "    Points: 5 \n",
    "\n",
    "(4) What did you find in the data?\n",
    "(please include figures or tables in the report, but no source code)\n",
    "    Points: 5 \n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "** Slides (for 10 minutes of presentation): Story-telling **\n",
    "    Points: 20\n",
    "\n",
    "\n",
    "1. Motivation about the data collection, why the topic is interesting to you.\n",
    "    Points: 5 \n",
    "\n",
    "2. Communicating Results (figure/table)\n",
    "    Points: 10 \n",
    "\n",
    "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
    "    Points: 5 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
